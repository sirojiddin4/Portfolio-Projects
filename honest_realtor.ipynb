{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2300f649",
   "metadata": {},
   "source": [
    "# Honest Realtor: Predicting Accurate Housing Prices in Uzbekistan\n",
    "\n",
    "## Project Overview\n",
    "The Honest Realtor project aims to build a machine learning model that accurately predicts housing prices in the Uzbek real estate market. Leveraging data scraped from open web pages, the project seeks to address pricing discrepancies and provide reliable price estimates for apartments in Uzbekistan.\n",
    "\n",
    "### Dataset\n",
    "The dataset consists of over 3,000 high-quality records sourced from online ads for apartment sales. These records include key features relevant to housing prices, such as location, size, number of rooms, and additional amenities. \n",
    "\n",
    "### Current Progress\n",
    "- **Data Collection:** Successfully scraped and cleaned data from online real estate platforms.\n",
    "- **Model Development:** Developed an initial machine learning model for price prediction, achieving an accuracy of 81%.\n",
    "\n",
    "### Goals and Next Steps\n",
    "1. Enhance the current model by:\n",
    "   - Incorporating additional features from the dataset.\n",
    "   - Exploring advanced machine learning algorithms.\n",
    "2. Evaluate and validate the model to improve accuracy beyond 81%.\n",
    "3. Create an intuitive interface for users to input apartment details and get price predictions.\n",
    "\n",
    "This project is a step toward building a transparent and accurate pricing tool for the real estate market in Uzbekistan.\n",
    "\n",
    "Dependencies\n",
    "To run this project, install the following dependencies:\n",
    "\n",
    "pandas\n",
    "numpy\n",
    "selenium\n",
    "beautifulsoup4\n",
    "requests\n",
    "scikit-learn\n",
    "webdriver-manager\n",
    "pickle (built-in with Python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8024f8-ccba-414b-87a9-4f85daa0a68f",
   "metadata": {},
   "source": [
    "### **Scraping Real Estate Listings**\n",
    "\n",
    "#### **Overview**\n",
    "This script extracts detailed information about real estate listings from a given list of URLs. It parses the webpage content using `BeautifulSoup`, handles missing data gracefully, and saves the extracted information into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09804924-eb4c-4789-8bb3-c64a138b3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping complete. Check the listing_data.csv file for the output.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_listing_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') if response.status_code == 200 else None\n",
    "    if not soup:\n",
    "        return None\n",
    "\n",
    "    # Initialize all information with 'Data missing' to handle missing data gracefully\n",
    "    data = {\n",
    "        \"Title\": 'Data missing',\n",
    "        \"Price\": 'Data missing',\n",
    "        \"Rooms\": 'Data missing',\n",
    "        \"Size\": 'Data missing',\n",
    "        \"Land Area\": 'Data missing',\n",
    "        \"Floor\": 'Data missing',\n",
    "        \"Condition\": 'Data missing',\n",
    "        \"Material\": 'Data missing',\n",
    "        \"Location\": 'Data missing'\n",
    "    }\n",
    "\n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1', class_='MuiTypography-root MuiTypography-h2 mui-style-1tyknu')\n",
    "    if title_tag:\n",
    "        data[\"Title\"] = title_tag.text.strip()\n",
    "\n",
    "    # Extract price\n",
    "    price_div = soup.find('div', class_='MuiTypography-root MuiTypography-h2 mui-style-86wpc3')\n",
    "    if price_div:\n",
    "        data[\"Price\"] = price_div.text.strip()\n",
    "\n",
    "    # Extract location\n",
    "    location_div = soup.find('div', class_='MuiTypography-root MuiTypography-body2 mui-style-31fjox')\n",
    "    if location_div:\n",
    "        data[\"Location\"] = location_div.text.strip()\n",
    "\n",
    "    # Mapping for labels to data keys\n",
    "    label_to_key = {\n",
    "        \"Комнат\": \"Rooms\",\n",
    "        \"Площадь\": \"Size\",\n",
    "        \"Площадь земли\": \"Land Area\",\n",
    "        \"Этаж\": \"Floor\",\n",
    "        \"Ремонт\": \"Condition\",\n",
    "        \"Материал\": \"Material\"\n",
    "    }\n",
    "\n",
    "    # Find all markers and the corresponding value\n",
    "    markers = soup.find_all('div', class_='MuiTypography-root MuiTypography-overline mui-style-1xqesu')\n",
    "    for marker in markers:\n",
    "        label = marker.text.strip()\n",
    "        key = label_to_key.get(label)\n",
    "        if key:\n",
    "            value_div = marker.find_next('div', class_='MuiTypography-root MuiTypography-subtitle2 mui-style-fu5la2')\n",
    "            if value_div:\n",
    "                data[key] = value_div.text.strip()\n",
    "\n",
    "    return [data[key] for key in [\"Title\", \"Price\", \"Rooms\", \"Size\", \"Land Area\", \"Floor\", \"Condition\", \"Material\", \"Location\"]] + [url]\n",
    "\n",
    "\n",
    "# Assuming listing_urls.txt contains the URLs\n",
    "urls = []\n",
    "with open('h_sales_listing_urls.txt', 'r') as file:\n",
    "    urls = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Scrape data for the first 100 URLs for testing\n",
    "data = []\n",
    "for url in urls:\n",
    "    listing_data = scrape_listing_data(url)\n",
    "    if listing_data:\n",
    "        data.append(listing_data)\n",
    "\n",
    "# Write the data to a CSV file\n",
    "headers = [\"Title\", \"Price\", \"Rooms\", \"Size\", \"Land Area\", \"Floor\", \"Condition\", \"Material\", \"Location\", \"URL\"]\n",
    "with open('listing_data2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"Data scraping complete. Check the listing_data.csv file for the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71827a4f-f792-41eb-8603-62e16dfde857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82be593",
   "metadata": {},
   "source": [
    "### **Processing Real Estate Data**\n",
    "\n",
    "#### **Overview**\n",
    "This script processes real estate data from a CSV file to clean and transform it into a more structured and analyzable format. The steps include removing unnecessary columns, cleaning text-based fields, creating new columns, and encoding categorical data into binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da920d9d-607d-424a-8593-76aaa2750ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Price Rooms   Size                                           Location  \\\n",
      "0    112000     5    100  город Ташкент, Алмазарский район, массив Бируни-1   \n",
      "1     78000     3     70    город Ташкент, Юнусабадский район, 18-й квартал   \n",
      "2     35500     2     50      город Ташкент, Чиланзарский район, Чиланзар-6   \n",
      "3    103500     2     70  город Ташкент, Яккасарайский район, махалля Ди...   \n",
      "4  27460.38     2  74.75  Сурхандарьинская область, Термез, улица Ислама...   \n",
      "\n",
      "                                 URL Current Floor Total Floors  \\\n",
      "0  https://uybor.uz/listings/1216080             4            4   \n",
      "1  https://uybor.uz/listings/1214327             2            4   \n",
      "2   https://uybor.uz/listings/799690             1            4   \n",
      "3  https://uybor.uz/listings/1214939             1           10   \n",
      "4  https://uybor.uz/listings/1220246             5            5   \n",
      "\n",
      "   Cond_Data missing  Cond_Авторский проект  Cond_Евроремонт  Cond_Средний  \\\n",
      "0              False                  False             True         False   \n",
      "1              False                  False            False          True   \n",
      "2              False                  False             True         False   \n",
      "3              False                  False             True         False   \n",
      "4              False                  False            False          True   \n",
      "\n",
      "   Cond_Требует ремонта  Cond_Черновая отделка  Mat_Data missing  Mat_Блочный  \\\n",
      "0                 False                  False             False        False   \n",
      "1                 False                  False             False        False   \n",
      "2                 False                  False              True        False   \n",
      "3                 False                  False             False        False   \n",
      "4                 False                  False             False        False   \n",
      "\n",
      "   Mat_Другое  Mat_Кирпичный  Mat_Монолитный  Mat_Панельный  Record ID  \n",
      "0       False           True           False          False          1  \n",
      "1       False           True           False          False          2  \n",
      "2       False          False           False          False          3  \n",
      "3       False           True           False          False          4  \n",
      "4       False           True           False          False          5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('listing_data.csv')\n",
    "\n",
    "# Create a copy of the DataFrame for safety\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Perform the specified operations on the copy\n",
    "\n",
    "# 1. Remove the 'Title' and 'Land Area' columns\n",
    "df_copy.drop(columns=['Title', 'Land Area'], inplace=True)\n",
    "\n",
    "# 2. Remove \" у.е.\" from the 'Price' and keep only the numbers (also remove spaces for consistency)\n",
    "df_copy['Price'] = df_copy['Price'].str.replace(' у.е.', '').str.replace(' ', '')\n",
    "\n",
    "# 3. Remove \" м²\" from the 'Size'\n",
    "df_copy['Size'] = df_copy['Size'].str.replace(' м²', '')\n",
    "\n",
    "# 4. Split 'Floor' into 'Current Floor' and 'Total Floors', then remove the original 'Floor' column\n",
    "df_copy[['Current Floor', 'Total Floors']] = df_copy['Floor'].str.split('/', expand=True)\n",
    "df_copy.drop(columns=['Floor'], inplace=True)\n",
    "\n",
    "# Ensure pandas is installed\n",
    "# !pip install pandas\n",
    "\n",
    "# Assuming df_copy is your current DataFrame after previous modifications\n",
    "\n",
    "# Convert 'Condition' and 'Material' columns into binary (one-hot encoded) columns\n",
    "condition_dummies = pd.get_dummies(df_copy['Condition'], prefix='Cond')\n",
    "material_dummies = pd.get_dummies(df_copy['Material'], prefix='Mat')\n",
    "\n",
    "# Concatenate the new binary columns with the original DataFrame\n",
    "df_copy = pd.concat([df_copy, condition_dummies, material_dummies], axis=1)\n",
    "\n",
    "# Drop the original 'Condition' and 'Material' columns as they are now encoded\n",
    "df_copy.drop(columns=['Condition', 'Material'], inplace=True)\n",
    "\n",
    "# Assign a unique ID to each record\n",
    "df_copy['Record ID'] = range(1, len(df_copy) + 1)\n",
    "\n",
    "\n",
    "# Optionally, save the modified DataFrame back to a new CSV file\n",
    "df_copy.to_csv('final_modified_data.csv', index=False)\n",
    "\n",
    "# Display the modified DataFrame (optional)\n",
    "print(df_copy.head())\n",
    "\n",
    "\n",
    "# Optionally, save the modified DataFrame back to a new CSV file\n",
    "df_copy.to_csv('modified_data.csv', index=False)\n",
    "\n",
    "# If you want to work with the modified data in Python, you can continue using df_copy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251bfe0-6b2d-44dd-b424-e2b70e9db35f",
   "metadata": {},
   "source": [
    "### Web Scraping and Data Enrichment\n",
    "\n",
    "#### Code Description\n",
    "The Python code enriches the dataset by scraping geographic coordinates (Longitude and Latitude) for property listings using Selenium WebDriver with Yandex Maps. Key functionalities include:\n",
    "- Loading the dataset and validating required columns.\n",
    "- Automating searches for property locations.\n",
    "- Extracting and saving coordinates into the dataset.\n",
    "- Handling errors to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d439d6-76d0-4718-926f-cf2d716c5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Setup the Selenium WebDriver using WebDriver Manager\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')  # Uncomment for headless mode\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "try:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv('modified_data.csv')\n",
    "\n",
    "    # Ensure the DataFrame has 'Longitude' and 'Latitude' columns\n",
    "    if 'Longitude' not in df.columns:\n",
    "        df['Longitude'] = None\n",
    "    if 'Latitude' not in df.columns:\n",
    "        df['Latitude'] = None\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for index, row in df.iloc[2001:3224].iterrows():\n",
    "        try:\n",
    "            driver.get(\"https://yandex.com/maps\")\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input.input__control\")))\n",
    "\n",
    "            search_box = driver.find_element(By.CSS_SELECTOR, \"input.input__control\")\n",
    "            search_box.clear()  # Ensure the search box is clear before typing\n",
    "            search_box.send_keys(row['Location'])\n",
    "            search_box.send_keys(Keys.ENTER)\n",
    "            WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \".toponym-card-title-view__coords-badge\")))\n",
    "\n",
    "            coordinates_element = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CSS_SELECTOR, \".toponym-card-title-view__coords-badge\"))\n",
    "            )\n",
    "            coordinates = coordinates_element.text.split(', ')\n",
    "            if len(coordinates) == 2:\n",
    "                longitude, latitude = coordinates\n",
    "                df.at[index, 'Longitude'] = longitude\n",
    "                df.at[index, 'Latitude'] = latitude\n",
    "            else:\n",
    "                raise ValueError(\"Coordinates format not recognized.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "            df.at[index, 'Longitude'] = 'Data Missing'\n",
    "            df.at[index, 'Latitude'] = 'Data Missing'\n",
    "\n",
    "    # Save the DataFrame after processing all rows\n",
    "    df.to_csv('updated_data_with_coordinates_6.csv', index=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc294d8-f302-4dbf-a1a3-a33a1c7ee207",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "This script trains a machine learning model to predict real estate prices \n",
    "in Uzbekistan. It includes:\n",
    "1. Loading and preprocessing the dataset.\n",
    "2. Splitting the data into training and test sets.\n",
    "3. Training a regression model (e.g., Linear Regression, Random Forest).\n",
    "4. Evaluating the model using metrics such as Mean Absolute Error (MAE) \n",
    "   and Mean Absolute Percentage Error (MAPE).\n",
    "5. Exporting predictions for analysis.\n",
    "6. Saving the trained model for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887708f-f262-4b9c-a8fa-36f5a69108dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Custom function to calculate Mean Absolute Percentage Error\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    # Avoid division by zero by replacing zeros with a small value\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    epsilon = np.finfo(np.float64).eps  # Small constant\n",
    "    y_true_modified = np.where(y_true == 0, epsilon, y_true)  # Replace zeros\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true_modified))\n",
    "    return mape\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"real_estate_data.csv\")\n",
    "\n",
    "# Preprocessing: Define features and target variable\n",
    "features = ['size', 'location', 'number_of_rooms', 'year_built']  # Example feature columns\n",
    "target = 'price'\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mape = 100 - mean_absolute_percentage_error(y_test, predictions) * 100  # Accuracy as (100 - MAPE)\n",
    "\n",
    "# Output evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Accuracy: {mape:.2f}%\")\n",
    "\n",
    "# Export predictions for analysis\n",
    "results = pd.DataFrame({'Actual': y_test, 'Predicted': predictions})\n",
    "results.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Results exported to test_predictions.csv.\")\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"trained_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)\n",
    "print(\"Trained model saved as 'trained_model.pkl'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
