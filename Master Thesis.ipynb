{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcd469-9bc9-4a6b-9586-22b99fffb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    " from dotenv import load_dotenv \n",
    "    import os \n",
    "    import pandas as pd \n",
    "    import csv \n",
    "\n",
    " \n",
    "\n",
    "    # Load environment variables from .env file \n",
    "    load_dotenv() \n",
    "\n",
    " \n",
    "\n",
    "    # Set OpenAI API key \n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    " \n",
    "\n",
    "    # Path to the CSV file with image names \n",
    "    csv_file = r\"C:\\Users\\user\\Desktop\\Thesis\\Stage 3 - Data Processing\\Sample\\sample_memes\\main_names.csv\" \n",
    "    START_INDEX = 0 \n",
    "    END_INDEX = 100 \n",
    "    image_names = pd.read_csv(csv_file)[\"image_name\"].tolist()[START_INDEX:END_INDEX] \n",
    "\n",
    " \n",
    "\n",
    "    # Open a CSV file for appending results \n",
    "    with open(\"sarcasm_classification_results.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile: \n",
    "        # Define the CSV writer \n",
    "        csv_writer = csv.writer(csvfile) \n",
    "         \n",
    "        # Write the headers \n",
    "        csv_writer.writerow([\"image_name\", \"Sarcasm_type\", \"Explanation\"]) \n",
    "\n",
    " \n",
    "\n",
    "        # Loop through the image names \n",
    "        for i, image_name in enumerate(image_names): \n",
    "            print(f\"Processing image {i + 1} of {len(image_names)}: {image_name}\") \n",
    "\n",
    " \n",
    "\n",
    "            image_url = f\"https://raw.githubusercontent.com/sirojiddin4/mathe_sample_test/main/{image_name}\" \n",
    "\n",
    " \n",
    "\n",
    "            prompt = f\"\"\" \n",
    "            Classify the following meme into one of the sarcasm groups below, considering both visual and textual information: \n",
    "\n",
    " \n",
    "\n",
    "            not_sarcastic \n",
    "            general \n",
    "            twisted_meaning \n",
    "            very_twisted \n",
    "\n",
    " \n",
    "\n",
    "            Return: \n",
    "            SarcasmGroup: One of the four categories. \n",
    "            Explanation: Max 20 words, no commas. \n",
    "\n",
    " \n",
    "\n",
    "            Output (CSV): \n",
    "            SarcasmGroup,Explanation \n",
    "\n",
    " \n",
    "\n",
    "            Rules: \n",
    "            No extra text or comments. \n",
    "            Follow the format exactly. \n",
    "            Don't include headers of csv \n",
    "            Don't include ``` \n",
    "                             \n",
    "            Example: \n",
    "            not_sarcastic,A heartfelt message about self-happiness and solitude \n",
    "            \"\"\"  \n",
    "                    # Make the API call \n",
    "        response = openai.ChatCompletion.create( \n",
    "            model=\"gpt-4o\", \n",
    "            messages=[ \n",
    "                { \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [ \n",
    "                        {\"type\": \"text\", \"text\": prompt}, \n",
    "                        { \n",
    "                            \"type\": \"image_url\", \n",
    "                            \"image_url\": { \n",
    "                                \"url\": image_url \n",
    "                            } \n",
    "                        } \n",
    "                    ] \n",
    "                } \n",
    "            ], \n",
    "            max_tokens=300, \n",
    "        ) \n",
    "\n",
    " \n",
    "\n",
    "        # Extract the response content \n",
    "        result = response.choices[0].message[\"content\"] \n",
    "        try: \n",
    "            # Split the result into SarcasmGroup and Explanation \n",
    "            sarcasm_group, explanation = result.split(\",\", 1) \n",
    "        except Exception as e: \n",
    "             # If splitting fails, assign fallback values \n",
    "            print(f\"Error splitting response for {image_name}: {e}\") \n",
    "            sarcasm_group = \"not identified\" \n",
    "            explanation = \"not identified\" \n",
    "\n",
    " \n",
    "\n",
    "        # Write the data into the CSV \n",
    "        csv_writer.writerow([image_name, sarcasm_group.strip(), explanation.strip()]) \n",
    "        print(f\"Saved result for {image_name}: {sarcasm_group}, {explanation}\") \n",
    "\n",
    " import openai \n",
    "from dotenv import load_dotenv \n",
    "import os \n",
    "import pandas as pd \n",
    "import csv \n",
    "\n",
    " \n",
    "\n",
    "# Load environment variables from .env file \n",
    "load_dotenv() \n",
    "\n",
    " \n",
    "\n",
    "# Set OpenAI API key \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    " \n",
    "\n",
    "# Path to the CSV file with textual data \n",
    "csv_file = r\"C:\\Users\\user\\Desktop\\Thesis\\Stage 3 - Data Processing\\Sample\\sample_data_humans.csv\" \n",
    "START_INDEX = 0 \n",
    "END_INDEX = 100 \n",
    "data = pd.read_csv(csv_file) \n",
    "\n",
    " \n",
    "\n",
    "# Open a CSV file for appending results \n",
    "with open(\"sarcasm_text_classification_results.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile: \n",
    "    # Define the CSV writer \n",
    "    csv_writer = csv.writer(csvfile) \n",
    "     \n",
    "    # Write the headers \n",
    "    csv_writer.writerow([\"image_name\", \"text_corrected\", \"sarcasm_type\", \"explanation\"]) \n",
    "\n",
    " \n",
    "\n",
    "    # Loop through the data \n",
    "    for i, row in data.iloc[START_INDEX:END_INDEX].iterrows(): \n",
    "        image_name = row[\"image_name\"] \n",
    "        text_corrected = row[\"text_corrected\"] \n",
    "        print(f\"Processing entry {i + 1} of {END_INDEX}: {image_name}\") \n",
    "\n",
    " \n",
    "\n",
    "        prompt = f\"\"\" \n",
    "        Classify the following text into one of the sarcasm groups below: \n",
    "         \n",
    "        Text: \"{text_corrected}\" \n",
    "\n",
    " \n",
    "\n",
    "        Sarcasm groups: \n",
    "        - not_sarcastic \n",
    "        - general \n",
    "        - twisted_meaning \n",
    "        - very_twisted \n",
    "\n",
    " \n",
    "\n",
    "        Return: \n",
    "        SarcasmType: One of the four categories. \n",
    "        Explanation: Max 20 words, no commas. \n",
    "\n",
    " \n",
    "\n",
    "        Output (CSV): \n",
    "        SarcasmType,Explanation \n",
    "\n",
    " \n",
    "\n",
    "        Rules: \n",
    "        No extra text or comments. \n",
    "        Follow the format exactly. \n",
    "        Don't include headers of csv \n",
    "        \"\"\"  \n",
    "\n",
    "         # Make the API call \n",
    "        try: \n",
    "            response = openai.ChatCompletion.create( \n",
    "                model=\"gpt-4o\", \n",
    "                messages=[ \n",
    "                    {\"role\": \"user\", \"content\": prompt} \n",
    "                ], \n",
    "                max_tokens=300, \n",
    "            ) \n",
    "\n",
    " \n",
    "\n",
    "            # Extract the response content \n",
    "            result = response.choices[0].message[\"content\"] \n",
    "             \n",
    "            # Split the result into SarcasmType and Explanation \n",
    "            sarcasm_type, explanation = result.split(\",\", 1) \n",
    "        except Exception as e: \n",
    "            # If something goes wrong, log the issue and assign fallback values \n",
    "            print(f\"Error processing {image_name}: {e}\") \n",
    "            sarcasm_type = \"not identified\" \n",
    "            explanation = \"not identified\" \n",
    "\n",
    " \n",
    "\n",
    "        # Write the data into the CSV \n",
    "        csv_writer.writerow([image_name, text_corrected, sarcasm_type.strip(), explanation.strip()]) \n",
    "        print(f\"Saved result for {image_name}: {sarcasm_type}, {explanation}\") \n",
    "\n",
    " \n",
    "\n",
    " # ============================================ \n",
    "#   IMPORT LIBRARIES \n",
    "# ============================================ \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    " \n",
    "\n",
    "# Scikit-learn for classification metrics \n",
    "from sklearn.metrics import ( \n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    cohen_kappa_score \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# SciPy and statsmodels for Chi-Squared and McNemar's Test \n",
    "from scipy.stats import chi2_contingency \n",
    "from statsmodels.stats.contingency_tables import mcnemar \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   LOAD DATA \n",
    "# ============================================ \n",
    "df = pd.read_csv(\"/kaggle/input/memotion-7k-ai-vs-human-annotations/combined_data.csv\") \n",
    "\n",
    " \n",
    "\n",
    "# Optional: create length columns (word counts) for text & explanations \n",
    "df[\"text_length\"] = df[\"text_corrected\"].astype(str).apply(lambda x: len(x.split())) \n",
    "df[\"multimodal_explanation_length\"] = df[\"multimodal_explanation_ai\"].astype(str).apply(lambda x: len(x.split())) \n",
    "df[\"unimodal_explanation_length\"] = df[\"unimodal_explanation_ai\"].astype(str).apply(lambda x: len(x.split())) \n",
    "\n",
    " \n",
    "\n",
    "# NOTE: Updated the label_set to match alphabetical order of classes \n",
    "#       (\"general\" likely comes before \"not_sarcastic\"). \n",
    "#       Verify with: sorted(df[\"multimodal_annotation_humans\"].unique()) \n",
    "label_set = [\"general\", \"not_sarcastic\", \"twisted_meaning\", \"very_twisted\"] \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   HELPER FUNCTIONS \n",
    "# ============================================ \n",
    "def calculate_overlap(ai_labels, human_labels): \n",
    "    \"\"\" \n",
    "    Calculate simple percentage overlap between AI and human labels. \n",
    "    \"\"\" \n",
    "    matches = (ai_labels == human_labels) \n",
    "    overlap_percent = 100.0 * matches.sum() / len(matches) \n",
    "    return overlap_percent \n",
    "\n",
    " \n",
    "\n",
    "def print_confusion_metrics(ai_labels, human_labels, label_set=None, desc=\"\"): \n",
    "    \"\"\" \n",
    "    Print confusion matrix, classification report, and Cohen's Kappa for AI vs. Human labels. \n",
    "    \"\"\" \n",
    "    print(f\"\\n=== {desc} ===\") \n",
    "\n",
    "     if label_set is not None: \n",
    "        cm = confusion_matrix(human_labels, ai_labels, labels=label_set) \n",
    "    else: \n",
    "        cm = confusion_matrix(human_labels, ai_labels) \n",
    "\n",
    " \n",
    "\n",
    "    cm_df = pd.DataFrame( \n",
    "        cm, \n",
    "        index=[f\"True: {label}\" for label in label_set], \n",
    "        columns=[f\"Predicted: {label}\" for label in label_set] \n",
    "    ) \n",
    "\n",
    " \n",
    "\n",
    "    print(\"Confusion Matrix:\") \n",
    "    print(cm_df) \n",
    "\n",
    " \n",
    "\n",
    "    print(\"\\nClassification Report:\") \n",
    "    if label_set is not None: \n",
    "        print(classification_report(human_labels, ai_labels, target_names=label_set, zero_division=0)) \n",
    "    else: \n",
    "        print(classification_report(human_labels, ai_labels, zero_division=0)) \n",
    "\n",
    " \n",
    "\n",
    "    kappa = cohen_kappa_score(human_labels, ai_labels) \n",
    "    print(f\"Cohen's Kappa: {kappa:.3f}\") \n",
    "\n",
    " \n",
    "\n",
    "def run_chi_squared_test(cat_var, numeric_or_cat_var, desc=\"\"): \n",
    "    \"\"\" \n",
    "    Perform Chi-Squared test between a categorical variable (cat_var) and \n",
    "    another variable (numeric_or_cat_var), which you can treat as categorical \n",
    "    (e.g. binned or direct word counts). \n",
    "    \"\"\" \n",
    "    print(f\"\\n--- Chi-Squared Test: {desc} ---\") \n",
    "\n",
    " \n",
    "\n",
    "    # Create a contingency table \n",
    "    contingency_table = pd.crosstab(cat_var, numeric_or_cat_var) \n",
    "    print(\"Contingency Table (first few rows/cols):\") \n",
    "    print(contingency_table.iloc[:10, :10])  # truncated if large \n",
    "\n",
    " \n",
    "\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table) \n",
    "\n",
    " \n",
    "\n",
    "    print(f\"\\nChi-Squared Statistic = {chi2:.3f}\") \n",
    "    print(f\"Degrees of Freedom = {dof}\") \n",
    "    print(f\"p-value = {p:.6f}\") \n",
    "\n",
    " \n",
    "\n",
    "    if p < 0.05: \n",
    "        print(\"=> Reject null hypothesis: There is a significant association.\") \n",
    "    else: \n",
    "        print(\"=> Fail to reject null hypothesis: No significant association.\") \n",
    "\n",
    " \n",
    "\n",
    "def run_mcnemar_test(unimodal_correct, multimodal_correct): \n",
    "    \"\"\" \n",
    "    Perform McNemar's test on two paired nominal outcomes: unimodal_correct vs. multimodal_correct. \n",
    "    \"\"\" \n",
    "    a = int((unimodal_correct & multimodal_correct).sum()) \n",
    "    b = int((unimodal_correct & ~multimodal_correct).sum()) \n",
    "    c = int((~unimodal_correct & multimodal_correct).sum()) \n",
    "    d = int((~unimodal_correct & ~multimodal_correct).sum()) \n",
    "\n",
    "   table = [[a, b], \n",
    "             [c, d]] \n",
    "\n",
    " \n",
    "\n",
    "    result = mcnemar(table, exact=False, correction=True) \n",
    "\n",
    " \n",
    "\n",
    "    print(\"\\n--- McNemar's Test ---\") \n",
    "    print(\"Contingency Table (Correct vs. Incorrect):\") \n",
    "    print(f\"                 Multimodal Correct   Multimodal Wrong\") \n",
    "    print(f\"Unimodal Correct          {a}                  {b}\") \n",
    "    print(f\"Unimodal Wrong            {c}                  {d}\") \n",
    "\n",
    " \n",
    "\n",
    "    print(f\"\\nStatistic = {result.statistic:.3f}, p-value = {result.pvalue:.6f}\") \n",
    "    if result.pvalue < 0.05: \n",
    "        print(\"=> Significant difference between unimodal and multimodal performance.\") \n",
    "    else: \n",
    "        print(\"=> No significant difference between unimodal and multimodal performance.\") \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   MULTIMODAL SECTION \n",
    "# ============================================ \n",
    "print(\"============================================\") \n",
    "print(\"      MULTIMODAL ANALYSIS\") \n",
    "print(\"============================================\") \n",
    "\n",
    " \n",
    "\n",
    "# 1) Sample Dataset \n",
    "print(\"\\n1) SAMPLE DATASET (Reason: to check correctness and integrity)\") \n",
    "display(df.head(5)) \n",
    "\n",
    " \n",
    "\n",
    "# 2) Overlap with Human Annotations \n",
    "print(\"\\n2) OVERLAP WITH HUMAN ANNOTATIONS (Reason: to get a brief overview)\") \n",
    "multi_overlap = calculate_overlap(df[\"multimodal_annotation_ai\"], df[\"multimodal_annotation_humans\"]) \n",
    "print(f\"Multimodal AI vs Human Overlap: {multi_overlap:.2f}%\") \n",
    "\n",
    " \n",
    "\n",
    "# 3) Cohen’s Kappa \n",
    "print(\"\\n3) COHEN’S KAPPA (Reason: to get the actual agreement rate considering chance)\") \n",
    "kappa_multi = cohen_kappa_score(df[\"multimodal_annotation_humans\"], df[\"multimodal_annotation_ai\"]) \n",
    "print(f\"Cohen's Kappa (Multimodal vs Human): {kappa_multi:.3f}\") \n",
    "\n",
    " \n",
    "\n",
    "# 4) Classification Report \n",
    "print(\"\\n4) CLASSIFICATION REPORT (Reason: to understand performance for each category)\") \n",
    "print( \n",
    "    classification_report( \n",
    "        df[\"multimodal_annotation_humans\"], \n",
    "        df[\"multimodal_annotation_ai\"], \n",
    "        target_names=label_set, \n",
    "        zero_division=0 \n",
    "    ) \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# 5) Confusion Matrix \n",
    "print(\"\\n5) CONFUSION MATRIX (Reason: to understand categorization patterns and biases)\") \n",
    "print_confusion_metrics( \n",
    "    ai_labels=df[\"multimodal_annotation_ai\"], \n",
    "    human_labels=df[\"multimodal_annotation_humans\"], \n",
    "    label_set=label_set, \n",
    "    desc=\"Multimodal AI vs. Human\" ) # 6) Chi-Squared: Multimodal Annotation VS text_corrected length \n",
    "print(\"\\n6) CHI-SQUARED TEST: MULTIMODAL ANNOTATION VS TEXT LENGTH (Reason: to see if input size affects \n",
    "outcome)\") \n",
    "run_chi_squared_test( \n",
    "    cat_var=df[\"multimodal_annotation_ai\"], \n",
    "    numeric_or_cat_var=df[\"text_length\"], \n",
    "    desc=\"Multimodal Annotation vs. Text Word Count\" \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# 7) Chi-Squared: Multimodal Annotation VS multimodal_explanation_ai length \n",
    "print(\"\\n7) CHI-SQUARED TEST: MULTIMODAL ANNOTATION VS EXPLANATION LENGTH (Reason: to see if label affects \n",
    "explanation)\") \n",
    "run_chi_squared_test( \n",
    "    cat_var=df[\"multimodal_annotation_ai\"], \n",
    "    numeric_or_cat_var=df[\"multimodal_explanation_length\"], \n",
    "    desc=\"Multimodal Annotation vs. Multimodal Explanation Length\" \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   UNIMODAL SECTION \n",
    "# ============================================ \n",
    "print(\"\\n\\n============================================\") \n",
    "print(\"      UNIMODAL ANALYSIS\") \n",
    "print(\"============================================\") \n",
    "\n",
    " \n",
    "\n",
    "# 1) Sample Dataset \n",
    "print(\"\\n1) SAMPLE DATASET (Reason: to check correctness and integrity)\") \n",
    "display(df.head(5)) \n",
    "\n",
    " \n",
    "\n",
    "# 2) Overlap with Human Annotations \n",
    "print(\"\\n2) OVERLAP WITH HUMAN ANNOTATIONS (Reason: to get a brief overview)\") \n",
    "uni_overlap = calculate_overlap(df[\"unimodal_annotation_ai\"], df[\"multimodal_annotation_humans\"]) \n",
    "print(f\"Unimodal AI vs Human Overlap: {uni_overlap:.2f}%\") \n",
    "\n",
    " \n",
    "\n",
    "# 3) Cohen’s Kappa \n",
    "print(\"\\n3) COHEN’S KAPPA (Reason: to get the actual agreement rate considering chance)\") \n",
    "kappa_uni = cohen_kappa_score(df[\"multimodal_annotation_humans\"], df[\"unimodal_annotation_ai\"]) \n",
    "print(f\"Cohen's Kappa (Unimodal vs Human): {kappa_uni:.3f}\") \n",
    "\n",
    " \n",
    "\n",
    "# 4) Classification Report \n",
    "print(\"\\n4) CLASSIFICATION REPORT (Reason: to understand performance for each category)\") \n",
    "print( \n",
    "    classification_report( \n",
    "        df[\"multimodal_annotation_humans\"], \n",
    "        df[\"unimodal_annotation_ai\"], \n",
    "        target_names=label_set, \n",
    "        zero_division=0 \n",
    "    ) \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# 5) Confusion Matrix \n",
    "print(\"\\n5) CONFUSION MATRIX (Reason: to understand categorization patterns and biases)\") \n",
    "print_confusion_metrics( \n",
    "    ai_labels=df[\"unimodal_annotation_ai\"], \n",
    "    human_labels=df[\"multimodal_annotation_humans\"], \n",
    "    label_set=label_set, \n",
    "        desc=\"Unimodal AI vs. Human\" \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# 6) Chi-Squared: Unimodal Annotation vs text_corrected length \n",
    "print(\"\\n6) CHI-SQUARED TEST: UNIMODAL ANNOTATION VS TEXT LENGTH (Reason: to see if input size affects outcome)\") \n",
    "run_chi_squared_test( \n",
    "    cat_var=df[\"unimodal_annotation_ai\"], \n",
    "    numeric_or_cat_var=df[\"text_length\"], \n",
    "    desc=\"Unimodal Annotation vs. Text Word Count\" \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# 7) Chi-Squared: Unimodal Annotation vs unimodal_explanation_ai length \n",
    "print(\"\\n7) CHI-SQUARED TEST: UNIMODAL ANNOTATION VS EXPLANATION LENGTH (Reason: to see if label affects \n",
    "explanation)\") \n",
    "run_chi_squared_test( \n",
    "    cat_var=df[\"unimodal_annotation_ai\"], \n",
    "    numeric_or_cat_var=df[\"unimodal_explanation_length\"], \n",
    "    desc=\"Unimodal Annotation vs. Unimodal Explanation Length\" \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   SPECIAL ANALYSIS: MCNEMAR'S TEST \n",
    "# ============================================ \n",
    "print(\"\\n\\n============================================\") \n",
    "print(\"      SPECIAL: MCNEMAR’S TEST\") \n",
    "print(\"============================================\") \n",
    "print(\"(Reason: to check if there is a significant difference between unimodal and multimodal annotations on the SAME \n",
    "items)\") \n",
    "\n",
    " \n",
    "\n",
    "unimodal_correct = (df[\"unimodal_annotation_ai\"] == df[\"multimodal_annotation_humans\"]) \n",
    "multimodal_correct = (df[\"multimodal_annotation_ai\"] == df[\"multimodal_annotation_humans\"]) \n",
    "\n",
    " \n",
    "\n",
    "run_mcnemar_test(unimodal_correct, multimodal_correct) \n",
    "\n",
    " \n",
    "\n",
    "print(\"\\nAll calculations are complete.\") \n",
    "                  # ============================================ \n",
    "#   IMPORT LIBRARIES FOR HEATMAPS \n",
    "# ============================================ \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix \n",
    "import pandas as pd \n",
    "\n",
    " \n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/memotion-7k-ai-vs-human-annotations/combined_data.csv\") \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   FUNCTION TO GENERATE AND SAVE HEATMAPS \n",
    "# ============================================ \n",
    "def save_confusion_matrix_heatmap(cm, labels, title, filename): \n",
    "    \"\"\" \n",
    "    Generate and save a heatmap for a confusion matrix. \n",
    "    \"\"\" \n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels) \n",
    "    plt.title(title) \n",
    "    plt.xlabel(\"Predicted\") \n",
    "    plt.ylabel(\"True\") \n",
    "    plt.savefig(filename, bbox_inches=\"tight\", dpi=300) \n",
    "    plt.close() \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   GENERATE AND SAVE HEATMAPS \n",
    "# ============================================ \n",
    "# Multimodal Confusion Matrix Heatmap \n",
    "cm_multi = confusion_matrix(df[\"multimodal_annotation_humans\"], df[\"multimodal_annotation_ai\"], labels=label_set) \n",
    "save_confusion_matrix_heatmap( \n",
    "    cm_multi, \n",
    "    label_set, \n",
    "    title=\"Multimodal AI vs Human Confusion Matrix\", \n",
    "    filename=\"multimodal_confusion_matrix.png\" \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "# Unimodal Confusion Matrix Heatmap \n",
    "cm_uni = confusion_matrix(df[\"multimodal_annotation_humans\"], df[\"unimodal_annotation_ai\"], labels=label_set) \n",
    "save_confusion_matrix_heatmap( \n",
    "    cm_uni, \n",
    "    label_set, \n",
    "    title=\"Unimodal AI vs Human Confusion Matrix\", \n",
    "    filename=\"unimodal_confusion_matrix.png\" \n",
    ") \n",
    "\n",
    " \n",
    "\n",
    "print(\"\\nHeatmaps for confusion matrices have been saved as 'multimodal_confusion_matrix.png' and \n",
    "'unimodal_confusion_matrix.png'.\") \n",
    "\n",
    " # ============================================ \n",
    "#   HISTOGRAMS & BASIC CORRELATION CHECKS \n",
    "# ============================================ \n",
    "\n",
    " \n",
    "\n",
    "df[\"text_length\"] = df[\"text_corrected\"].astype(str).apply(lambda x: len(x.split())) \n",
    "df[\"multimodal_explanation_length\"] = df[\"multimodal_explanation_ai\"].astype(str).apply(lambda x: len(x.split())) \n",
    "df[\"unimodal_explanation_length\"] = df[\"unimodal_explanation_ai\"].astype(str).apply(lambda x: len(x.split())) \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "# ============================================ \n",
    "#   PRINT AVERAGE LENGTHS PER LABEL \n",
    "# ============================================ \n",
    "\n",
    " \n",
    "\n",
    "# A) Average Text Length per Label \n",
    "print(\"=== Average Text Length by Multimodal Classification ===\") \n",
    "print(df.groupby(\"multimodal_annotation_ai\")[\"text_length\"].mean()) \n",
    "print(\"\\n=== Average Text Length by Unimodal Classification ===\") \n",
    "print(df.groupby(\"unimodal_annotation_ai\")[\"text_length\"].mean()) \n",
    "\n",
    " \n",
    "\n",
    "# B) Average Explanation Length per Label \n",
    "print(\"\\n=== Average Multimodal Explanation Length by Multimodal Classification ===\") \n",
    "print(df.groupby(\"multimodal_annotation_ai\")[\"multimodal_explanation_length\"].mean()) \n",
    "print(\"\\n=== Average Unimodal Explanation Length by Unimodal Classification ===\") \n",
    "print(df.groupby(\"unimodal_annotation_ai\")[\"unimodal_explanation_length\"].mean()) \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
